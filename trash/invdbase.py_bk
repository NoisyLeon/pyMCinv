# -*- coding: utf-8 -*-
"""
A python module for joint inversion based on ASDF database

:Methods:


:Dependencies:
    pyasdf and its dependencies
    ObsPy  and its dependencies
    pyproj
    Basemap
    pyfftw 0.10.3 (optional)
    
:Copyright:
    Author: Lili Feng
    Graduate Research Assistant
    CIEI, Department of Physics, University of Colorado Boulder
    email: lili.feng@colorado.edu
"""
import pyasdf, h5py
import numpy as np
import matplotlib.pyplot as plt
import obspy
import warnings
import copy
import os, shutil
from functools import partial
import multiprocessing
from subprocess import call
from mpl_toolkits.basemap import Basemap, shiftgrid, cm, interp
import obspy
import vprofile, mcpost
import time
import numpy.ma as ma
import field2d_earth
from pyproj import Geod

class invASDF(pyasdf.ASDFDataSet):
    """ An object to for MCMC inversion based on ASDF database
    =================================================================================================================
    version history:
           - first version
    =================================================================================================================
    """
    def print_info(self):
        """
        print information of the dataset.
        """
        outstr  = '================================================= Ambient Noise Cross-correlation Database =================================================\n'
        outstr  += self.__str__()+'\n'
        outstr  += '--------------------------------------------------------------------------------------------------------------------------------------------\n'
        if 'NoiseXcorr' in self.auxiliary_data.list():
            outstr      += 'NoiseXcorr              - Cross-correlation seismogram\n'
        if 'StaInfo' in self.auxiliary_data.list():
            outstr      += 'StaInfo                 - Auxiliary station information\n'
        if 'DISPbasic1' in self.auxiliary_data.list():
            outstr      += 'DISPbasic1              - Basic dispersion curve, no jump correction\n'
        if 'DISPbasic2' in self.auxiliary_data.list():
            outstr      += 'DISPbasic2              - Basic dispersion curve, with jump correction\n'
        if 'DISPpmf1' in self.auxiliary_data.list():
            outstr      += 'DISPpmf1                - PMF dispersion curve, no jump correction\n'
        if 'DISPpmf2' in self.auxiliary_data.list():
            outstr      += 'DISPpmf2                - PMF dispersion curve, with jump correction\n'
        if 'DISPbasic1interp' in self.auxiliary_data.list():
            outstr      += 'DISPbasic1interp        - Interpolated DISPbasic1\n'
        if 'DISPbasic2interp' in self.auxiliary_data.list():
            outstr      += 'DISPbasic2interp        - Interpolated DISPbasic2\n'
        if 'DISPpmf1interp' in self.auxiliary_data.list():
            outstr      += 'DISPpmf1interp          - Interpolated DISPpmf1\n'
        if 'DISPpmf2interp' in self.auxiliary_data.list():
            outstr      += 'DISPpmf2interp          - Interpolated DISPpmf2\n'
        if 'FieldDISPbasic1interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPbasic1interp   - Field data of DISPbasic1\n'
        if 'FieldDISPbasic2interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPbasic2interp   - Field data of DISPbasic2\n'
        if 'FieldDISPpmf1interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPpmf1interp     - Field data of DISPpmf1\n'
        if 'FieldDISPpmf2interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPpmf2interp     - Field data of DISPpmf2\n'
        outstr += '============================================================================================================================================\n'
        print outstr
        return
    
    def _get_lon_lat_arr(self, path, hd=True):
        """Get longitude/latitude array
        """
        minlon                  = self.auxiliary_data['Header'][path].parameters['minlon']
        maxlon                  = self.auxiliary_data['Header'][path].parameters['maxlon']
        minlat                  = self.auxiliary_data['Header'][path].parameters['minlat']
        maxlat                  = self.auxiliary_data['Header'][path].parameters['maxlat']
        if not hd:
            dlon                = self.auxiliary_data['Header'][path].parameters['dlon']
            dlat                = self.auxiliary_data['Header'][path].parameters['dlat']
        else:
            dlon                = self.auxiliary_data['Header'][path].parameters['dlon_HD']
            dlat                = self.auxiliary_data['Header'][path].parameters['dlat_HD']
        self.lons               = np.arange(int((maxlon-minlon)/dlon)+1)*dlon+minlon
        self.lats               = np.arange(int((maxlat-minlat)/dlat)+1)*dlat+minlat
        self.Nlon               = self.lons.size
        self.Nlat               = self.lats.size
        self.lonArr, self.latArr= np.meshgrid(self.lons, self.lats)
        return
    
    def read_ref_dbase(self, inasdfname, phase='P'):
        """
        read radial receiver function data from input ASDF file
        ==========================================================================
        ::: input :::
        inasdfname  - input ASDF file name
        phase       - default - P, P receiver function
        ::: output :::

        ==========================================================================
        """
        indset      = pyasdf.ASDFDataSet(inasdfname)
        #--------------------
        # station inventory
        #--------------------
        wavlst      = indset.waveforms.list()
        self.inv    = indset.waveforms[wavlst[0]].StationXML
        for staid in wavlst[1:]:
            self.inv+= indset.waveforms[staid].StationXML
        self.add_stationxml(self.inv)
        #--------------------
        # ref data
        #--------------------
        for staid in wavlst:
            netcode, stacode    = staid.split('.')
            staid_aux           = netcode+'_'+stacode+'_'+phase
            if indset.auxiliary_data.RefRHScount[staid_aux].parameters['Nhs'] == 0:
                print 'No harmonic stripping data for '+staid
                continue
            ref_header          = {'Nraw': indset.auxiliary_data['RefRHScount'][staid_aux].parameters['Nraw'], \
                                    'Nhs': indset.auxiliary_data['RefRHScount'][staid_aux].parameters['Nhs'], \
                                    'delta': indset.auxiliary_data['RefRHSmodel'][staid_aux]['A0_A1_A2']['A0'].parameters['delta'], \
                                    'npts': indset.auxiliary_data['RefRHSmodel'][staid_aux]['A0_A1_A2']['A0'].parameters['npts']}
            """
            0       - A0 from A0-A1-A2 inversion
            1       - misfit from raw A0+A1+A2
            2       - misfit from binned A0+A1+A2
            3       - weighted misfit from binned A0+A1+A2
            """
            data                = np.zeros((4, ref_header['npts']))
            data[0, :]          = indset.auxiliary_data['RefRHSmodel'][staid_aux]['A0_A1_A2']['A0'].data.value
            data[1, :]          = indset.auxiliary_data['RefRHSmodel'][staid_aux]['A0_A1_A2']['mf_A0_A1_A2_obs'].data.value
            data[2, :]          = indset.auxiliary_data['RefRHSmodel'][staid_aux]['A0_A1_A2']['mf_A0_A1_A2_bin'].data.value
            data[3, :]          = indset.auxiliary_data['RefRHSmodel'][staid_aux]['A0_A1_A2']['wmf_A0_A1_A2_bin'].data.value
            self.add_auxiliary_data(data=data, data_type='RefR', path=staid_aux, parameters=ref_header)
        return
    
    def read_raytomo_dbase(self, inh5fname, runid, dtype='ph', wtype='ray', create_header=True, Tmin=-999, Tmax=999, verbose=False):
        """
        read ray tomography data base
        =================================================================================
        ::: input :::
        inh5fname   - input hdf5 file name
        runid       - id of run for the ray tomography
        dtype       - data type (ph or gr)
        wtype       - wave type (ray or lov)
        Tmin, Tmax  - minimum and maximum period to extract from the tomographic results
        =================================================================================
        """
        if dtype is not 'ph' and dtype is not 'gr':
            raise ValueError('data type can only be ph or gr!')
        if wtype is not 'ray' and wtype is not 'lov':
            raise ValueError('wave type can only be ray or lov!')
        stalst      = self.waveforms.list()
        if len(stalst) == 0:
            print 'Inversion with surface wave datasets only, not added yet!'
            return
        indset          = h5py.File(inh5fname)
        #--------------------------------------------
        # header information from input hdf5 file
        #--------------------------------------------
        dataid          = 'reshaped_qc_run_'+str(runid)
        pers            = indset.attrs['period_array']
        grp             = indset[dataid]
        isotropic       = grp.attrs['isotropic']
        org_grp         = indset['qc_run_'+str(runid)]
        minlon          = indset.attrs['minlon']
        maxlon          = indset.attrs['maxlon']
        minlat          = indset.attrs['minlat']
        maxlat          = indset.attrs['maxlat']
        if isotropic:
            print 'isotropic inversion results do not output gaussian std!'
            return
        dlon_HD         = org_grp.attrs['dlon_HD']
        dlat_HD         = org_grp.attrs['dlat_HD']
        dlon            = org_grp.attrs['dlon']
        dlat            = org_grp.attrs['dlat']
        if create_header:
            inv_header  = {'minlon': minlon, 'maxlon': maxlon, 'minlat': minlat, 'maxlat': maxlat,
                           'dlon': dlon, 'dlat': dlat, 'dlon_HD': dlon_HD, 'dlat_HD': dlat_HD}
            self.add_auxiliary_data(data=np.array([]), data_type='Header', path='raytomo', parameters=inv_header)
        self._get_lon_lat_arr(path='raytomo', hd=True)
        for staid in stalst:
            netcode, stacode    = staid.split('.')
            staid_aux           = netcode+'_'+stacode
            stla, elev, stlo    = self.waveforms[staid].coordinates.values()
            if stlo < 0.:
                stlo            += 360.
            if stla > maxlat or stla < minlat or stlo > maxlon or stlo < minlon:
                print 'WARNING: station: '+ staid+', lat = '+str(stla)+' lon = '+str(stlo)+', out of the range of tomograpic maps!'
                continue
            disp_v              = np.array([])
            disp_un             = np.array([])
            T                   = np.array([])
            #-----------------------------
            # determine the indices
            #-----------------------------
            ind_lon             = np.where(stlo<=self.lons)[0][0]
            find_lon            = ind_lon            
            ind_lat             = np.where(stla<=self.lats)[0][0]
            find_lat            = ind_lat
            # point 1
            distmin, az, baz    = obspy.geodetics.gps2dist_azimuth(stla, stlo, self.lats[ind_lat], self.lons[ind_lon]) # distance is in m
            # point 2
            dist, az, baz       = obspy.geodetics.gps2dist_azimuth(stla, stlo, self.lats[ind_lat], self.lons[ind_lon-1]) # distance is in m
            if dist < distmin:
                find_lon        = ind_lon-1
                distmin         = dist
            # point 3
            dist, az, baz       = obspy.geodetics.gps2dist_azimuth(stla, stlo, self.lats[ind_lat-1], self.lons[ind_lon]) # distance is in m
            if dist < distmin:
                find_lat        = ind_lat-1
                distmin         = dist
            # point 4
            dist, az, baz       = obspy.geodetics.gps2dist_azimuth(stla, stlo, self.lats[ind_lat-1], self.lons[ind_lon-1]) # distance is in m
            if dist < distmin:
                find_lat        = ind_lat-1
                find_lon        = ind_lon-1
                distmin         = dist
            for per in pers:
                if per < Tmin or per > Tmax:
                    continue
                try:
                    pergrp      = grp['%g_sec'%( per )]
                    vel         = pergrp['vel_iso_HD'].value
                    vel_sem     = pergrp['vel_sem_HD'].value
                except KeyError:
                    if verbose:
                        print 'No data for T = '+str(per)+' sec'
                    continue
                T               = np.append(T, per)
                disp_v          = np.append(disp_v, vel[find_lat, find_lon])
                disp_un         = np.append(disp_un, vel_sem[find_lat, find_lon])
            data                = np.zeros((3, T.size))
            data[0, :]          = T[:]
            data[1, :]          = disp_v[:]
            data[2, :]          = disp_un[:]
            disp_header         = {'Np': T.size}
            self.add_auxiliary_data(data=data, data_type='RayDISPcurve', path=wtype+'/'+dtype+'/'+staid_aux, parameters=disp_header)
        indset.close()
        return
    
    def read_crust_thickness(self, infname='crsthk.xyz', source='crust_1.0'):
        """
        read crust thickness from a txt file (crust 1.0 model)
        """
        inArr       = np.loadtxt(infname)
        lonArr      = inArr[:, 0]
        lonArr      = lonArr.reshape(lonArr.size/360, 360)
        latArr      = inArr[:, 1]
        latArr      = latArr.reshape(latArr.size/360, 360)
        depthArr    = inArr[:, 2]
        depthArr    = depthArr.reshape(depthArr.size/360, 360)
        stalst      = self.waveforms.list()
        if len(stalst) == 0:
            print 'Inversion with surface wave datasets only, not added yet!'
            return
        for staid in stalst:
            netcode, stacode    = staid.split('.')
            staid_aux           = netcode+'_'+stacode
            stla, elev, stlo    = self.waveforms[staid].coordinates.values()
            if stlo > 180.:
                stlo            -= 360.
            whereArr= np.where((lonArr>=stlo)*(latArr>=stla))
            ind_lat = whereArr[0][-1]
            ind_lon = whereArr[1][0]
            # check
            lon     = lonArr[ind_lat, ind_lon]
            lat     = latArr[ind_lat, ind_lon]
            if abs(lon-stlo) > 1. or abs(lat - stla) > 1.:
                print 'ERROR!',lon,lat,stlo,stla
            depth   = depthArr[ind_lat, ind_lon]
            header  = {'moho_depth': depth, 'data_source': source}
            self.add_auxiliary_data(data=np.array([]), data_type='MohoDepth', path=staid_aux, parameters=header)
        return
    
    def read_sediment_thickness(self, infname='sedthk.xyz'):
        """
        read sediment thickness from a txt file (crust 1.0 model)
        """
        inArr   = np.loadtxt(infname)
        lonArr  = inArr[:, 0]
        lonArr  = lonArr.reshape(lonArr.size/360, 360)
        latArr  = inArr[:, 1]
        latArr  = latArr.reshape(latArr.size/360, 360)
        depthArr= inArr[:, 2]
        depthArr= depthArr.reshape(depthArr.size/360, 360)
        stalst                  = self.waveforms.list()
        if len(stalst) == 0:
            print 'Inversion with surface wave datasets only, not added yet!'
            return
        for staid in stalst:
            netcode, stacode    = staid.split('.')
            staid_aux           = netcode+'_'+stacode
            stla, elev, stlo    = self.waveforms[staid].coordinates.values()
            if stlo > 180.:
                stlo            -= 360.
            whereArr= np.where((lonArr>=stlo)*(latArr>=stla))
            ind_lat = whereArr[0][-1]
            ind_lon = whereArr[1][0]
            # check
            lon     = lonArr[ind_lat, ind_lon]
            lat     = latArr[ind_lat, ind_lon]
            if abs(lon-stlo) > 1. or abs(lat - stla) > 1.:
                print 'ERROR!',lon,lat,stlo,stla
            depth   = depthArr[ind_lat, ind_lon]
            header  = {'sedi_depth': depth, 'data_source': 'crust_1.0'}
            self.add_auxiliary_data(data=np.array([]), data_type='SediDepth', path=staid_aux, parameters=header)
        return
    
    def read_CU_model(self, infname='CU_SDT1.0.mod.h5'):
        """
        read reference model from a hdf5 file (CU Global Vs model)
        """
        indset      = h5py.File(infname)
        lons        = np.mgrid[0.:359.:2.]
        lats        = np.mgrid[-88.:89.:2.]
        stalst                  = self.waveforms.list()
        if len(stalst) == 0:
            print 'Inversion with surface wave datasets only, not added yet!'
            return
        for staid in stalst:
            netcode, stacode    = staid.split('.')
            staid_aux           = netcode+'_'+stacode
            stla, elev, stlo    = self.waveforms[staid].coordinates.values()
            if stlo < 0.:
                stlo            += 360.
            try:
                ind_lon         = np.where(lons>=stlo)[0][0]
            except:
                ind_lon         = lons.size - 1
            try:
                ind_lat         = np.where(lats>=stla)[0][0]
            except:
                ind_lat         = lats.size - 1
            pind                = 0
            while(True):
                if pind == 0:
                    data        = indset[str(lons[ind_lon])+'_'+str(lats[ind_lat])].value
                    if data[0, 1] != 0:
                        outlon  = lons[ind_lon]
                        outlat  = lats[ind_lat]
                        break
                    pind        += 1
                    continue
                data            = indset[str(lons[ind_lon+pind])+'_'+str(lats[ind_lat])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon+pind]
                    outlat      = lats[ind_lat]
                    break
                data            = indset[str(lons[ind_lon-pind])+'_'+str(lats[ind_lat])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon-pind]
                    outlat      = lats[ind_lat]
                    break
                data            = indset[str(lons[ind_lon])+'_'+str(lats[ind_lat+pind])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon]
                    outlat      = lats[ind_lat+pind]
                    break
                data            = indset[str(lons[ind_lon])+'_'+str(lats[ind_lat-pind])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon]
                    outlat      = lats[ind_lat-pind]
                    break
                data            = indset[str(lons[ind_lon-pind])+'_'+str(lats[ind_lat-pind])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon-pind]
                    outlat      = lats[ind_lat-pind]
                    break
                data            = indset[str(lons[ind_lon-pind])+'_'+str(lats[ind_lat+pind])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon-pind]
                    outlat      = lats[ind_lat+pind]
                    break
                data            = indset[str(lons[ind_lon+pind])+'_'+str(lats[ind_lat-pind])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon+pind]
                    outlat      = lats[ind_lat-pind]
                    break
                data            = indset[str(lons[ind_lon+pind])+'_'+str(lats[ind_lat+pind])].value
                if data[0, 1] != 0:
                    outlon      = lons[ind_lon+pind]
                    outlat      = lats[ind_lat+pind]
                    break
                pind            += 1
            if pind >= 5:
                print 'WARNING: Large differences in the finalized points: lon = '+str(outlon)+', lat = '+str(outlat)\
                    + ', station: '+staid+' stlo = '+str(stlo) + ', stla = '+str(stla)
            # print outlon, outlat, stlo, stla, pind
            header  = {'data_source': 'CU_SDT',\
                       'depth': 0, 'vs': 1, 'vsv': 2, 'vsh': 3, 'vsmin': 4, 'vsvmin': 5, 'vshmin': 6, \
                       'vsmax': 7, 'vsvmax': 8, 'vshmax': 9}
            self.add_auxiliary_data(data=data, data_type='ReferenceModel', path=staid_aux, parameters=header)
        return
    
    def mc_inv_iso(self, instafname=None, ref=True, phase=True, group=False, outdir='./workingdir', wdisp=0.2, rffactor=40.,\
                   monoc=True, verbose=False, step4uwalk=1500, numbrun=10000, subsize=1000, nprocess=None, parallel=True):
        """
        Bayesian Monte Carlo joint inversion of receiver function and surface wave data for an isotropic model
        ==================================================================================================================
        ::: input :::
        instafname  - input station list file indicating the stations for joint inversion
        ref         - include receiver function data or not
        phase/group - include phase/group velocity dispersion data or not
        outdir      - output directory
        wdisp       - weight of dispersion curve data (0. ~ 1.)
        rffactor    - factor for downweighting the misfit for likelihood computation of rf
        monoc       - require monotonical increase in the crust or not
        step4uwalk  - step interval for uniform random walk in the parameter space
        numbrun     - total number of runs
        subsize     - size of subsets, used if the number of elements in the parallel list is too large to avoid deadlock
        nprocess    - number of process
        parallel    - run the inversion in parallel or not 
        ==================================================================================================================
        """
        if not os.path.isdir(outdir):
            os.makedirs(outdir)
        if instafname is None:
            stalst  = self.waveforms.list()
        else:
            stalst  = []
            with open(instafname, 'r') as fid:
                for line in fid.readlines():
                    sline   = line.split()
                    if sline[2] == '1':
                        stalst.append(sline[0])
        if not ref and wdisp != 1.:
            wdisp   = 1.
            print 'wdisp is forced to be 1. for inversion without receiver function data'
        if phase and group:
            dispdtype   = 'both'
        elif phase and not group:
            dispdtype   = 'ph'
        else:
            dispdtype   = 'gr'
        ista        = 0
        Nsta        = len(stalst)
        for staid in stalst:
            netcode, stacode    = staid.split('.')
            staid_aux           = netcode+'_'+stacode
            stla, elev, stlo    = self.waveforms[staid].coordinates.values()
            #-----------------------------
            # get data
            #-----------------------------
            vpr                 = vprofile.vprofile1d()
            if phase:
                try:
                    indisp      = self.auxiliary_data['RayDISPcurve']['ray']['ph'][staid_aux].data.value
                    vpr.get_disp(indata=indisp, dtype='ph', wtype='ray')
                except KeyError:
                    print 'WARNING: No phase dispersion data for station: '+staid
            if group:
                try:
                    indisp      = self.auxiliary_data['RayDISPcurve']['ray']['gr'][staid_aux].data.value
                    vpr.get_disp(indata=indisp, dtype='gr', wtype='ray')
                except KeyError:
                    print 'WARNING: No group dispersion data for station: '+staid
            if vpr.data.dispR.npper == 0 and vpr.data.dispR.ngper == 0:
                print 'WARNING: No dispersion data for station: '+staid 
                continue
            if ref:
                try:
                    inrf        = self.auxiliary_data['RefR'][staid_aux+'_P'].data.value
                    N           = self.auxiliary_data['RefR'][staid_aux+'_P'].parameters['npts']
                    dt          = self.auxiliary_data['RefR'][staid_aux+'_P'].parameters['delta']
                    indata      = np.zeros((3, N))
                    indata[0, :]= np.arange(N)*dt
                    indata[1, :]= inrf[0, :]
                    indata[2, :]= inrf[3, :]
                    vpr.get_rf(indata = indata)
                except KeyError:
                    print 'WARNING: No receiver function data for station: '+staid
            #-----------------------------
            # initial model parameters
            #-----------------------------
            vsdata              = self.auxiliary_data['ReferenceModel'][staid_aux].data.value
            crtthk              = self.auxiliary_data['MohoDepth'][staid_aux].parameters['moho_depth']
            sedthk              = self.auxiliary_data['SediDepth'][staid_aux].parameters['sedi_depth']
            vpr.model.isomod.parameterize_input(zarr=vsdata[:, 0], vsarr=vsdata[:, 1], crtthk=crtthk, sedthk=sedthk, maxdepth=200.)
            vpr.getpara()
            ista                += 1
            # if staid != 'AK.HDA': continue
            # # # if np.random.rand() > 0.9:
            # # #     print staid
            # # #     return vpr, vsdata
            # # # else:
            # # #     continue
            print '--- Joint MC inversion for station: '+staid+' '+str(ista)+'/'+str(Nsta)
            if parallel:
                vpr.mc_joint_inv_iso_mp(outdir=outdir, dispdtype=dispdtype, wdisp=wdisp, rffactor=rffactor,\
                   monoc=monoc, pfx=staid, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun, subsize=subsize, nprocess=nprocess)
            else:
                vpr.mc_joint_inv_iso(outdir=outdir, dispdtype=dispdtype, wdisp=wdisp, rffactor=rffactor,\
                   monoc=monoc, pfx=staid, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
            # vpr.mc_joint_inv_iso(outdir=outdir, wdisp=wdisp, rffactor=rffactor,\
            #        monoc=monoc, pfx=staid, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
            # if staid == 'AK.COLD':
            #     return vpr
        return
    
class invhdf5(h5py.File):
    """ An object to for MCMC inversion based on HDF5 database
    =================================================================================================================
    version history:
           - first version
    =================================================================================================================
    """
    def print_info(self):
        """
        print information of the dataset.
        """
        outstr  = '================================================= Ambient Noise Cross-correlation Database =================================================\n'
        outstr  += self.__str__()+'\n'
        outstr  += '--------------------------------------------------------------------------------------------------------------------------------------------\n'
        if 'NoiseXcorr' in self.auxiliary_data.list():
            outstr      += 'NoiseXcorr              - Cross-correlation seismogram\n'
        if 'StaInfo' in self.auxiliary_data.list():
            outstr      += 'StaInfo                 - Auxiliary station information\n'
        if 'DISPbasic1' in self.auxiliary_data.list():
            outstr      += 'DISPbasic1              - Basic dispersion curve, no jump correction\n'
        if 'DISPbasic2' in self.auxiliary_data.list():
            outstr      += 'DISPbasic2              - Basic dispersion curve, with jump correction\n'
        if 'DISPpmf1' in self.auxiliary_data.list():
            outstr      += 'DISPpmf1                - PMF dispersion curve, no jump correction\n'
        if 'DISPpmf2' in self.auxiliary_data.list():
            outstr      += 'DISPpmf2                - PMF dispersion curve, with jump correction\n'
        if 'DISPbasic1interp' in self.auxiliary_data.list():
            outstr      += 'DISPbasic1interp        - Interpolated DISPbasic1\n'
        if 'DISPbasic2interp' in self.auxiliary_data.list():
            outstr      += 'DISPbasic2interp        - Interpolated DISPbasic2\n'
        if 'DISPpmf1interp' in self.auxiliary_data.list():
            outstr      += 'DISPpmf1interp          - Interpolated DISPpmf1\n'
        if 'DISPpmf2interp' in self.auxiliary_data.list():
            outstr      += 'DISPpmf2interp          - Interpolated DISPpmf2\n'
        if 'FieldDISPbasic1interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPbasic1interp   - Field data of DISPbasic1\n'
        if 'FieldDISPbasic2interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPbasic2interp   - Field data of DISPbasic2\n'
        if 'FieldDISPpmf1interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPpmf1interp     - Field data of DISPpmf1\n'
        if 'FieldDISPpmf2interp' in self.auxiliary_data.list():
            outstr      += 'FieldDISPpmf2interp     - Field data of DISPpmf2\n'
        outstr += '============================================================================================================================================\n'
        print outstr
        return
    
    def _get_lon_lat_arr(self):
        """Get longitude/latitude array
        """
        minlon          = self.attrs['minlon']
        maxlon          = self.attrs['maxlon']
        minlat          = self.attrs['minlat']
        maxlat          = self.attrs['maxlat']
        dlon            = self.attrs['dlon']
        dlat            = self.attrs['dlat']
        self.lons       = np.arange(int((maxlon-minlon)/dlon)+1)*dlon+minlon
        self.lats       = np.arange(int((maxlat-minlat)/dlat)+1)*dlat+minlat
        self.Nlon       = self.lons.size
        self.Nlat       = self.lats.size
        self.lonArr, self.latArr= np.meshgrid(self.lons, self.lats)
        return
    
    def read_raytomo_dbase(self, inh5fname, runid, dtype='ph', wtype='ray', create_header=True, Tmin=-999, Tmax=999, verbose=False, res='LD'):
        """
        read ray tomography data base
        =================================================================================
        ::: input :::
        inh5fname   - input hdf5 file name
        runid       - id of run for the ray tomography
        dtype       - data type (ph or gr)
        wtype       - wave type (ray or lov)
        Tmin, Tmax  - minimum and maximum period to extract from the tomographic results
        res         - resolution for grid points, default is LD, low-definition
        =================================================================================
        """
        if dtype is not 'ph' and dtype is not 'gr':
            raise ValueError('data type can only be ph or gr!')
        if wtype is not 'ray' and wtype is not 'lov':
            raise ValueError('wave type can only be ray or lov!')
        indset          = h5py.File(inh5fname)
        #--------------------------------------------
        # header information from input hdf5 file
        #--------------------------------------------
        dataid          = 'reshaped_qc_run_'+str(runid)
        pers            = indset.attrs['period_array']
        grp             = indset[dataid]
        isotropic       = grp.attrs['isotropic']
        org_grp         = indset['qc_run_'+str(runid)]
        minlon          = indset.attrs['minlon']
        maxlon          = indset.attrs['maxlon']
        minlat          = indset.attrs['minlat']
        maxlat          = indset.attrs['maxlat']
        if isotropic:
            print 'isotropic inversion results do not output gaussian std!'
            return
        if res == 'LD':
            sfx         = '_LD'
        elif res == 'HD':
            sfx         = '_HD'
        else:
            sfx         = ''
        dlon_interp     = org_grp.attrs['dlon'+sfx]
        dlat_interp     = org_grp.attrs['dlat'+sfx]
        dlon            = org_grp.attrs['dlon']
        dlat            = org_grp.attrs['dlat']
        if sfx == '':
            mask        = indset[dataid+'/mask1']
        else:
            mask        = indset[dataid+'/mask'+sfx]
        if create_header:
            self.attrs.create(name = 'minlon', data=minlon, dtype='f')
            self.attrs.create(name = 'maxlon', data=maxlon, dtype='f')
            self.attrs.create(name = 'minlat', data=minlat, dtype='f')
            self.attrs.create(name = 'maxlat', data=maxlat, dtype='f')
            self.attrs.create(name = 'dlon', data=dlon_interp)
            self.attrs.create(name = 'dlat', data=dlon_interp)
        self._get_lon_lat_arr()
        self.attrs.create(name='mask', data = mask)
        for ilat in range(self.Nlat):
            for ilon in range(self.Nlon):
                data_str    = str(self.lons[ilon])+'_'+str(self.lats[ilat])
                group       = self.create_group( name = data_str )
                disp_v      = np.array([])
                disp_un     = np.array([])
                T           = np.array([])
                for per in pers:
                    if per < Tmin or per > Tmax:
                        continue
                    try:
                        pergrp      = grp['%g_sec'%( per )]
                        vel         = pergrp['vel_iso'+sfx].value
                        vel_sem     = pergrp['vel_sem'+sfx].value
                    except KeyError:
                        if verbose:
                            print 'No data for T = '+str(per)+' sec'
                        continue
                    T               = np.append(T, per)
                    disp_v          = np.append(disp_v, vel[ilat, ilon])
                    disp_un         = np.append(disp_un, vel_sem[ilat, ilon])
                data                = np.zeros((3, T.size))
                data[0, :]          = T[:]
                data[1, :]          = disp_v[:]
                data[2, :]          = disp_un[:]
                group.create_dataset(name='disp_'+dtype+'_'+wtype, data=data)
                group.attrs.create(name='mask', data = mask[ilat, ilon])
        indset.close()
        return
    
    def read_crust_thickness(self, infname='crsthk.xyz', source='crust_1.0'):
        """
        read crust thickness from a txt file (crust 1.0 model)
        """
        inArr       = np.loadtxt(infname)
        lonArr      = inArr[:, 0]
        lonArr      = lonArr.reshape(lonArr.size/360, 360)
        latArr      = inArr[:, 1]
        latArr      = latArr.reshape(latArr.size/360, 360)
        depthArr    = inArr[:, 2]
        depthArr    = depthArr.reshape(depthArr.size/360, 360)
        for grp_id in self.keys():
            grp     = self[grp_id]
            split_id= grp_id.split('_')
            grd_lon = float(split_id[0])
            if grd_lon > 180.:
                grd_lon     -= 360.
            grd_lat = float(split_id[1])
            whereArr= np.where((lonArr>=grd_lon)*(latArr>=grd_lat))
            ind_lat = whereArr[0][-1]
            ind_lon = whereArr[1][0]
            # check
            lon     = lonArr[ind_lat, ind_lon]
            lat     = latArr[ind_lat, ind_lon]
            if abs(lon-grd_lon) > 1. or abs(lat - grd_lat) > 1.:
                print 'ERROR!', lon, lat, grd_lon, grd_lat
            depth   = depthArr[ind_lat, ind_lon]
            grp.attrs.create(name='crust_thk', data=depth)
            grp.attrs.create(name='crust_thk_source', data=source)
        return
    
    def read_sediment_thickness(self, infname='sedthk.xyz', source='crust_1.0'):
        """
        read sediment thickness from a txt file (crust 1.0 model)
        """
        inArr       = np.loadtxt(infname)
        lonArr      = inArr[:, 0]
        lonArr      = lonArr.reshape(lonArr.size/360, 360)
        latArr      = inArr[:, 1]
        latArr      = latArr.reshape(latArr.size/360, 360)
        depthArr    = inArr[:, 2]
        depthArr    = depthArr.reshape(depthArr.size/360, 360)
        for grp_id in self.keys():
            grp     = self[grp_id]
            split_id= grp_id.split('_')
            grd_lon = float(split_id[0])
            if grd_lon > 180.:
                grd_lon     -= 360.
            grd_lat = float(split_id[1])
            whereArr= np.where((lonArr>=grd_lon)*(latArr>=grd_lat))
            ind_lat = whereArr[0][-1]
            ind_lon = whereArr[1][0]
            # check
            lon     = lonArr[ind_lat, ind_lon]
            lat     = latArr[ind_lat, ind_lon]
            if abs(lon-grd_lon) > 1. or abs(lat - grd_lat) > 1.:
                print 'ERROR!', lon, lat, grd_lon, grd_lat
            depth   = depthArr[ind_lat, ind_lon]
            grp.attrs.create(name='sedi_thk', data=depth)
            grp.attrs.create(name='sedi_thk_source', data=source)
        return
    
    def read_CU_model(self, infname='CU_SDT1.0.mod.h5'):
        """
        read reference model from a hdf5 file (CU Global Vs model)
        """
        indset      = h5py.File(infname)
        lons        = np.mgrid[0.:359.:2.]
        lats        = np.mgrid[-88.:89.:2.]
        for grp_id in self.keys():
            grp         = self[grp_id]
            split_id    = grp_id.split('_')
            grd_lon     = float(split_id[0])
            if grd_lon < 0.:
                grd_lon += 360.
            grd_lat = float(split_id[1])
            try:
                ind_lon         = np.where(lons>=grd_lon)[0][0]
            except:
                ind_lon         = lons.size - 1
            try:
                ind_lat         = np.where(lats>=grd_lat)[0][0]
            except:
                ind_lat         = lats.size - 1
            if lons[ind_lon] - grd_lon > 1. and ind_lon > 0:
                ind_lon         -= 1
            if lats[ind_lat] - grd_lat > 1. and ind_lat > 0:
                ind_lat         -= 1
            if abs(lons[ind_lon] - grd_lon) > 1. or abs(lats[ind_lat] - grd_lat) > 1.:
                print 'ERROR!', lons[ind_lon], lats[ind_lat] , grd_lon, grd_lat
            data        = indset[str(lons[ind_lon])+'_'+str(lats[ind_lat])].value
            grp.create_dataset(name='reference_vs', data=data)
        indset.close()
        return
    
    def read_etopo(self, infname='../ETOPO2v2g_f4.nc', download=True, delete=True, source='etopo2'):
        """
        read topography data from etopo2 
        """
        from netCDF4 import Dataset
        try:
            etopodbase  = Dataset(infname)
        except IOError:
            if download:
                url     = 'https://www.ngdc.noaa.gov/mgg/global/relief/ETOPO2/ETOPO2v2-2006/ETOPO2v2g/netCDF/ETOPO2v2g_f4_netCDF.zip'
                os.system('wget '+url)
                os.system('unzip ETOPO2v2g_f4_netCDF.zip')
                if delete:
                    os.remove('ETOPO2v2g_f4_netCDF.zip')
                etopodbase  = Dataset('./ETOPO2v2g_f4.nc')
            else:
                print 'No etopo data!'
                return
        etopo       = etopodbase.variables['z'][:]
        lons        = etopodbase.variables['x'][:]
        lats        = etopodbase.variables['y'][:]
        for grp_id in self.keys():
            grp     = self[grp_id]
            split_id= grp_id.split('_')
            grd_lon = float(split_id[0])
            if grd_lon > 180.:
                grd_lon     -= 360.
            grd_lat = float(split_id[1])
            try:
                ind_lon         = np.where(lons>=grd_lon)[0][0]
            except:
                ind_lon         = lons.size - 1
            try:
                ind_lat         = np.where(lats>=grd_lat)[0][0]
            except:
                ind_lat         = lats.size - 1
            if lons[ind_lon] - grd_lon > (1./60.):
                ind_lon         -= 1
            if lats[ind_lat] - grd_lat > (1./60.):
                ind_lat         -= 1
            if abs(lons[ind_lon] - grd_lon) > 1./60. or abs(lats[ind_lat] - grd_lat) > 1./60.:
                print 'ERROR!', lons[ind_lon], lats[ind_lat] , grd_lon, grd_lat
            z                   = etopo[ind_lat, ind_lon]/1000. # convert to km
            grp.attrs.create(name='topo', data=z)
            grp.attrs.create(name='etopo_source', data=source)
        if delete and os.path.isfile('./ETOPO2v2g_f4.nc'):
            os.remove('./ETOPO2v2g_f4.nc')
        return
    
    def mc_inv_iso(self, ingrdfname=None, phase=True, group=False, outdir='./workingdir', vp_water=1.5, monoc=True,
            verbose=False, step4uwalk=1500, numbrun=15000, subsize=1000, nprocess=None, parallel=True, skipmask=True):
        """
        Bayesian Monte Carlo inversion of surface wave data for an isotropic model
        ==================================================================================================================
        ::: input :::
        ingrdfname  - input grid point list file indicating the grid points for surface wave inversion
        phase/group - include phase/group velocity dispersion data or not
        outdir      - output directory
        vp_water    - P wave velocity in water layer (default - 1.5 km/s)
        monoc       - require monotonical increase in the crust or not
        step4uwalk  - step interval for uniform random walk in the parameter space
        numbrun     - total number of runs
        subsize     - size of subsets, used if the number of elements in the parallel list is too large to avoid deadlock
        nprocess    - number of process
        parallel    - run the inversion in parallel or not
        skipmask    - skip masked grid points or not
        ==================================================================================================================
        """
        if not os.path.isdir(outdir):
            os.makedirs(outdir)
        if ingrdfname is None:
            grdlst  = self.keys()
        else:
            grdlst  = []
            with open(ingrdfname, 'r') as fid:
                for line in fid.readlines():
                    sline   = line.split()
                    lon     = float(sline[0])
                    if lon < 0.:
                        lon += 360.
                    if sline[2] == '1':
                        grdlst.append(str(lon)+'_'+sline[1])
        if phase and group:
            dispdtype   = 'both'
        elif phase and not group:
            dispdtype   = 'ph'
        else:
            dispdtype   = 'gr'
        igrd        = 0
        Ngrd        = len(grdlst)
        for grd_id in grdlst:
            split_id= grd_id.split('_')
            grd_lon = float(split_id[0])
            if grd_lon > 180.:
                grd_lon     -= 360.
            grd_lat = float(split_id[1])
            igrd    += 1
            if self[grd_id].attrs['mask'] and skipmask:
                print '--- Skip MC inversion for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd)
                continue
            #-----------------------------
            # get data
            #-----------------------------
            vpr                 = vprofile.vprofile1d()
            if phase:
                try:
                    indisp      = self[grd_id+'/disp_ph_ray'].value
                    vpr.get_disp(indata=indisp, dtype='ph', wtype='ray')
                except KeyError:
                    print 'WARNING: No phase dispersion data for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)
            if group:
                try:
                    indisp      = self[grd_id+'/disp_gr_ray'].value
                    vpr.get_disp(indata=indisp, dtype='gr', wtype='ray')
                except KeyError:
                    print 'WARNING: No group dispersion data for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)
            if vpr.data.dispR.npper == 0 and vpr.data.dispR.ngper == 0:
                print 'WARNING: No dispersion data for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)
                continue
            #-----------------------------
            # initial model parameters
            #-----------------------------
            vsdata              = self[grd_id+'/reference_vs'].value
            crtthk              = self[grd_id].attrs['crust_thk']
            sedthk              = self[grd_id].attrs['sedi_thk']
            topovalue           = self[grd_id].attrs['topo']
            
            vpr.model.isomod.parameterize_input(zarr=vsdata[:, 0], vsarr=vsdata[:, 1], crtthk=crtthk, sedthk=sedthk,\
                            topovalue=topovalue, maxdepth=200., vp_water=vp_water)
            vpr.getpara()
            # # # if np.random.rand() > 0.9 and topovalue<0.:
            # # #     print grd_id
            # # #     return vpr, vsdata
            # # # else:
            # # #     continue
            # # # if not (np.random.rand() > 0.9 and topovalue<0.):
            # # #     continue
            print '--- MC inversion for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd)
            if parallel:
                vpr.mc_joint_inv_iso_mp(outdir=outdir, dispdtype=dispdtype, wdisp=1.,\
                   monoc=monoc, pfx=grd_id, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun, subsize=subsize, nprocess=nprocess)
            else:
                vpr.mc_joint_inv_iso(outdir=outdir, dispdtype=dispdtype, wdisp=1., \
                   monoc=monoc, pfx=grd_id, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
            # # # return
        return
    
    def read_inv(self, datadir, ingrdfname=None, factor=1., thresh=0.5, skipmask=True):
        """
        read the inversion results in to data base
        ==================================================================================================================
        ::: input :::
        datadir     - data directory
        ingrdfname  - input grid point list file indicating the grid points for surface wave inversion
        factor      - factor to determine the threshhold value for selectingthe finalized model
        thresh      - threshhold value for selecting the finalized model
                        misfit < min_misfit*factor + thresh
        skipmask    - skip masked grid points or not
        ==================================================================================================================
        """
        if ingrdfname is None:
            grdlst  = self.keys()
        else:
            grdlst  = []
            with open(ingrdfname, 'r') as fid:
                for line in fid.readlines():
                    sline   = line.split()
                    lon     = float(sline[0])
                    if lon < 0.:
                        lon += 360.
                    if sline[2] == '1':
                        grdlst.append(str(lon)+'_'+sline[1])
        igrd        = 0
        Ngrd        = len(grdlst)
        for grd_id in grdlst:
            split_id= grd_id.split('_')
            grd_lon = float(split_id[0])
            if grd_lon > 180.:
                grd_lon     -= 360.
            grd_lat = float(split_id[1])
            igrd    += 1
            grp     = self[grd_id]
            if grp.attrs['mask'] and skipmask:
                print '--- Skipping inversion results for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd)
                continue
            print '--- Reading inversion results for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd)
            invfname = datadir+'/mc_inv.'+ grd_id+'.npz'
            datafname= datadir+'/mc_data.'+grd_id+'.npz'
            if not (os.path.isfile(invfname) and os.path.isfile(datafname)):
                raise ValueError('No inversion results for grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd))
            topovalue   = grp.attrs['topo']
            vpr         = mcpost.postvpr(waterdepth=-topovalue, factor=factor, thresh=thresh)
            vpr.read_inv_data(infname = invfname, verbose=False)
            vpr.read_data(infname = datafname)
            vpr.get_paraval()
            vpr.run_avg_fwrd(wdisp=1.)
            #------------------------------------------
            # store inversion results in the database
            #------------------------------------------
            grp.create_dataset(name = 'avg_paraval', data = vpr.avg_paraval)
            grp.create_dataset(name = 'min_paraval', data = vpr.min_paraval)
            grp.create_dataset(name = 'sem_paraval', data = vpr.sem_paraval)
            grp.attrs.create(name = 'avg_misfit', data = vpr.vprfwrd.data.misfit)
            grp.attrs.create(name = 'min_misfit', data = vpr.min_misfit)
        return
    
    def get_paraval(self, pindex, dtype='avg', ingrdfname=None, isthk=False):
        self._get_lon_lat_arr()
        data        = np.zeros(self.latArr.shape)
        mask        = np.ones(self.latArr.shape, dtype=bool)
        if ingrdfname is None:
            grdlst  = self.keys()
        else:
            grdlst  = []
            with open(ingrdfname, 'r') as fid:
                for line in fid.readlines():
                    sline   = line.split()
                    lon     = float(sline[0])
                    if lon < 0.:
                        lon += 360.
                    if sline[2] == '1':
                        grdlst.append(str(lon)+'_'+sline[1])
        igrd            = 0
        Ngrd            = len(grdlst)
        for grd_id in grdlst:
            split_id    = grd_id.split('_')
            grd_lon     = float(split_id[0])
            grd_lat     = float(split_id[1])
            igrd        += 1
            grp         = self[grd_id]
            try:
                ind_lon = np.where(grd_lon==self.lons)[0][0]
                ind_lat = np.where(grd_lat==self.lats)[0][0]
            except IndexError:
                # print 'WARNING: grid data N/A at: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd)
                continue
            try:
                paraval             = grp[dtype+'_paraval'].value
            except KeyError:
                # print 'WARNING: no data at grid: lon = '+str(grd_lon)+', lat = '+str(grd_lat)+', '+str(igrd)+'/'+str(Ngrd)
                continue
            data[ind_lat, ind_lon]  = paraval[pindex]
            if isthk:
                topovalue               = grp.attrs['topo']
                data[ind_lat, ind_lon]  = data[ind_lat, ind_lon] - topovalue
            mask[ind_lat, ind_lon]      = False
        if not np.allclose(mask, self.attrs['mask']):
            print 'WARNING: check the mask array!'
        return data, mask
    
    def get_filled_paraval(self, pindex, dtype='avg', ingrdfname=None, isthk=False):
        minlon      = self.attrs['minlon']
        maxlon      = self.attrs['maxlon']
        minlat      = self.attrs['minlat']
        maxlat      = self.attrs['maxlat']
        data, mask  = self.get_paraval(pindex=pindex, dtype=dtype, ingrdfname=ingrdfname, isthk=isthk)
        ind_valid   = np.logical_not(mask)
        data_out    = data.copy()
        g           = Geod(ellps='WGS84')
        vlonArr     = self.lonArr[ind_valid]
        vlatArr     = self.latArr[ind_valid]
        vdata       = data[ind_valid]
        L           = vlonArr.size
        for ilat in range(self.Nlat):
            for ilon in range(self.Nlon):
                if not mask[ilat, ilon]:
                    continue
                clonArr         = np.ones(L, dtype=float)*self.lons[ilon]
                clatArr         = np.ones(L, dtype=float)*self.lats[ilat]
                az, baz, dist   = g.inv(clonArr, clatArr, vlonArr, vlatArr)
                ind_min         = dist.argmin()
                data_out[ilat, ilon] \
                                = vdata[ind_min]
        return data_out
    
    def fill_paraval(self, dtype='avg'):
        grp                 = self.create_group( name = dtype+'_paraval' )
        for pindex in range(13):
            if pindex == 11 or pindex == 12:
                data_out    = self.get_filled_paraval(dtype=dtype, isthk=True)
            else:
                data_out    = self.get_filled_paraval(dtype=dtype, isthk=False)
        
        #         
        #         
        #         imin = dist.argmin(); mindist = dist[imin]
        #         elat = elatArr[imin]; elon = elonArr[imin]
        #         cname = '%g'%(elon) + '_%g' %(elat)
        #         cArr = self[outname][cname][...]
        #         if mindist < Dref: outArr=((Dref-mindist)*cArr+mindist*avgArr)/Dref
        #             # outArr=npr.evaluate ( '((Dref-mindist)*cArr+mindist*avgArr)/Dref ')
        #         else: outArr=avgArr
        #         dset = self[outname].create_dataset( name=name, shape=outArr.shape, data=outArr)
        #         dset.attrs.create(name = 'lon', data=lon, dtype='f')
        #         dset.attrs.create(name = 'lat', data=lat, dtype='f')
        #         # save to txt file
        #         if outdir !=None:
        #             if not os.path.isdir(outdir): os.makedirs(outdir)
        #             np.savetxt(outdir+'/'+name+'_mod', outArr, fmt='%g')
        #         
        # index_valid = np.logical_not(mask)
        # datain      = data[index_valid]
        # lonin       = self.lonArr[index_valid]
        # latin       = self.latArr[index_valid]
        # 
        # 
        # 
        #         avgArr = self[modelname].attrs['avg_model'][...]
        # latarr = minlat + np.arange( (maxlat-minlat)/dlat + 1)*dlat
        # lonarr = minlon + np.arange( (maxlon-minlon)/dlon + 1)*dlon
        # outname = modelname+sfx
        # try: del self[outname]
        # except: pass
        # self.copy( source = modelname, dest = outname )
        # elonArr = np.array([]); elatArr = np.array([])
        # for index in self[outname].keys():
        #     elat = self[outname][index].attrs['lat']
        #     elon = self[outname][index].attrs['lon']
        #     elonArr = np.append(elonArr, elon)
        #     elatArr = np.append(elatArr, elat)
        # L=elonArr.size
        # g = Geod(ellps='WGS84')
        # print '================ Start horizontal extrapolation =============='
        # for lon in lonarr:
        #     for lat in latarr:
        #         name='%g'%(lon) + '_%g' %(lat)
        #         if (name in self[outname].keys()): continue
        #         print 'Extending to ', name
        #         clonArr=np.ones(L)*lon; clatArr=np.ones(L)*lat
        #         az, baz, dist = g.inv(clonArr, clatArr, elonArr, elatArr)
        #         imin = dist.argmin(); mindist = dist[imin]
        #         elat = elatArr[imin]; elon = elonArr[imin]
        #         cname = '%g'%(elon) + '_%g' %(elat)
        #         cArr = self[outname][cname][...]
        #         if mindist < Dref: outArr=((Dref-mindist)*cArr+mindist*avgArr)/Dref
        #             # outArr=npr.evaluate ( '((Dref-mindist)*cArr+mindist*avgArr)/Dref ')
        #         else: outArr=avgArr
        #         dset = self[outname].create_dataset( name=name, shape=outArr.shape, data=outArr)
        #         dset.attrs.create(name = 'lon', data=lon, dtype='f')
        #         dset.attrs.create(name = 'lat', data=lat, dtype='f')
        #         # save to txt file
        #         if outdir !=None:
        #             if not os.path.isdir(outdir): os.makedirs(outdir)
        #             np.savetxt(outdir+'/'+name+'_mod', outArr, fmt='%g')
        # self[outname].attrs.create(name = 'minlat', data=minlat, dtype='f')
        # self[outname].attrs.create(name = 'maxlat', data=maxlat, dtype='f')
        # self[outname].attrs.create(name = 'dlat', data=dlat, dtype='f')
        # self[outname].attrs.create(name = 'minlon', data=minlon, dtype='f')
        # self[outname].attrs.create(name = 'maxlon', data=maxlon, dtype='f')
        # self[outname].attrs.create(name = 'dlon', data=dlon, dtype='f')
        # print '================ End horizontal extrapolation =============='
        # 
        # #--------------------------------------------------
        # # interpolation for parameter in the paraval array
        # #--------------------------------------------------
        # field2d_interp  = field2d_earth.Field2d(minlon=minlon, maxlon=maxlon, dlon=dlon,
        #                     minlat=minlat, maxlat=maxlat, dlat=dlat, period=10., evlo=(minlon+maxlon)/2., evla=(minlat+maxlat)/2.)
        # field2d_interp.read_array(lonArr = lonin, latArr = latin, ZarrIn = datain)
        # outfname        = 'interp_paraval.lst'
        # field2d_interp.interp_nearneighbor(workingdir=workingdir, outfname=outfname)
        # dataout         = field2d_interp.Zarr
        
        # return dataout, index_valid, data

        # mdata       = ma.masked_array(data, mask=mask )
        # data_out    = interp(mdata, xin=self.lons, yin=self.lats, xout=self.lonArr, yout=self.latArr, order=1, masked=False)
        #-----------
        # plot data
        #-----------
        # m           = self._get_basemap(projection=projection)
        # x, y        = m(self.lonArr, self.latArr)
        # cmap        = 'cv'
        # if cmap == 'ses3d':
        #     cmap        = colormaps.make_colormap({0.0:[0.1,0.0,0.0], 0.2:[0.8,0.0,0.0], 0.3:[1.0,0.7,0.0],0.48:[0.92,0.92,0.92],
        #                     0.5:[0.92,0.92,0.92], 0.52:[0.92,0.92,0.92], 0.7:[0.0,0.6,0.7], 0.8:[0.0,0.0,0.8], 1.0:[0.0,0.0,0.1]})
        # elif cmap == 'cv':
        #     import pycpt
        #     cmap    = pycpt.load.gmtColormap('./cv.cpt')
        # else:
        #     try:
        #         if os.path.isfile(cmap):
        #             import pycpt
        #             cmap    = pycpt.load.gmtColormap(cmap)
        #     except:
        #         pass
        # im          = m.pcolormesh(x, y, data_out, cmap=cmap, shading='gouraud', vmin=vmin, vmax=vmax)
        # cb          = m.colorbar(im, "bottom", size="3%", pad='2%')
        # cb.set_label(clabel, fontsize=12, rotation=0)
        # cb.ax.tick_params(labelsize=15)
        # cb.set_alpha(1)
        # cb.draw_all()
        # # # cb.solids.set_rasterized(True)
        # cb.solids.set_edgecolor("face")
        # # m.shadedrelief(scale=1., origin='lower')
        # # if showfig:
        # plt.show()
        # plot 
        # return data_out
    
    def smooth_paraval(self, pindex, dtype='avg', ingrdfname=None, isthk=False):
        data, mask  = self.get_paraval(pindex=pindex, dtype=dtype, ingrdfname=ingrdfname, isthk=isthk)
        data_smooth = np.zeros(data.shape)
        
        #- Loop over subvolumes.---------------------------------------------------
        for n in np.arange(self.nsubvol):
            if modelname == 'dvsv':
                v_filtered = self.m[n].dvsv 
            if modelname == 'dvsh':
                v_filtered = self.m[n].dvsh 
            if modelname == 'drho':
                v_filtered = self.m[n].drho 
            if modelname == 'dvp':
                v_filtered = self.m[n].dvp
            v = np.copy(v_filtered)
            #- Size of the array.
            nx=len(self.m[n].lat)-1
            ny=len(self.m[n].lon)-1
            nz=len(self.m[n].r)-1
            #- Gaussian smoothing. --------------------------------------------------
            if filter_type=='gauss':
                #- Estimate element width.
                r=np.mean(self.m[n].r)
                dx=r*np.pi*(self.m[n].lat[0]-self.m[n].lat[1])/180.0
                #- Colat and lon fields for the small Gaussian.
                dn=3*np.ceil(sigma/dx)
                nx_min=np.round(float(nx)/2.0)-dn
                nx_max=np.round(float(nx)/2.0)+dn
                ny_min=np.round(float(ny)/2.0)-dn
                ny_max=np.round(float(ny)/2.0)+dn
                lon,colat=np.meshgrid(self.m[n].lon[ny_min:ny_max],90.0-self.m[n].lat[nx_min:nx_max])
                colat=np.pi*colat/180.0
                lon=np.pi*lon/180.0
                #- Volume element.
                dy=r*np.pi*np.sin(colat)*(self.m[n].lon[1]-self.m[n].lon[0])/180.0
                dV=dx*dy
                #- Unit vector field.
                x=np.cos(lon)*np.sin(colat)
                y=np.sin(lon)*np.sin(colat)
                z=np.cos(colat)
                #- Make a Gaussian centred in the middle of the grid. -----------------
                i=np.round(float(nx)/2.0)-1
                j=np.round(float(ny)/2.0)-1
                colat_i=np.pi*(90.0-self.m[n].lat[i])/180.0
                lon_j=np.pi*self.m[n].lon[j]/180.0
                x_i=np.cos(lon_j)*np.sin(colat_i)
                y_j=np.sin(lon_j)*np.sin(colat_i)
                z_k=np.cos(colat_i)
                #- Compute the Gaussian.
                G=x*x_i+y*y_j+z*z_k
                G=G/np.max(np.abs(G))
                G=r*np.arccos(G)
                G=np.exp(-0.5*G**2/sigma**2)/(2.0*np.pi*sigma**2)
                #- Move the Gaussian across the field. --------------------------------
                for i in np.arange(dn+1,nx-dn-1):
                    for j in np.arange(dn+1,ny-dn-1):
                        for k in np.arange(nz):
                            v_filtered[i,j,k]=np.sum(v[i-dn:i+dn,j-dn:j+dn,k]*G*dV)
            #- Smoothing by averaging over neighbouring cells. ----------------------
            elif filter_type=='neighbour':
                for iteration in np.arange(int(sigma)):
                    for i in np.arange(1,nx-1):
                        for j in np.arange(1,ny-1):
                            v_filtered[i,j,:]=(v[i,j,:]+v[i+1,j,:]+v[i-1,j,:]+v[i,j+1,:]+v[i,j-1,:])/5.0
        
    def _get_basemap(self, projection='lambert', geopolygons=None, resolution='i'):
        """Get basemap for plotting results
        """
        # fig=plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')
        plt.figure()
        minlon      = self.attrs['minlon']
        maxlon      = self.attrs['maxlon']
        minlat      = self.attrs['minlat']
        maxlat      = self.attrs['maxlat']
        lat_centre  = (maxlat+minlat)/2.0
        lon_centre  = (maxlon+minlon)/2.0
        if projection=='merc':
            m       = Basemap(projection='merc', llcrnrlat=minlat-5., urcrnrlat=maxlat+5., llcrnrlon=minlon-5.,
                      urcrnrlon=maxlon+5., lat_ts=20, resolution=resolution, epsg = 4269)
            # m.drawparallels(np.arange(minlat,maxlat,dlat), labels=[1,0,0,1])
            # m.drawmeridians(np.arange(minlon,maxlon,dlon), labels=[1,0,0,1])
            m.drawparallels(np.arange(-80.0,80.0,5.0), labels=[1,0,0,1])
            m.drawmeridians(np.arange(-170.0,170.0,5.0), labels=[1,0,0,1])
            m.drawstates(color='g', linewidth=2.)
        elif projection=='global':
            m       = Basemap(projection='ortho',lon_0=lon_centre, lat_0=lat_centre, resolution=resolution)
            # m.drawparallels(np.arange(-80.0,80.0,10.0), labels=[1,0,0,1])
            # m.drawmeridians(np.arange(-170.0,170.0,10.0), labels=[1,0,0,1])
        elif projection=='regional_ortho':
            m1      = Basemap(projection='ortho', lon_0=minlon, lat_0=minlat, resolution='l')
            m       = Basemap(projection='ortho', lon_0=minlon, lat_0=minlat, resolution=resolution,\
                        llcrnrx=0., llcrnry=0., urcrnrx=m1.urcrnrx/mapfactor, urcrnry=m1.urcrnry/3.5)
            m.drawparallels(np.arange(-80.0,80.0,10.0), labels=[1,0,0,0],  linewidth=2,  fontsize=20)
            # m.drawparallels(np.arange(-90.0,90.0,30.0),labels=[1,0,0,0], dashes=[10, 5], linewidth=2,  fontsize=20)
            # m.drawmeridians(np.arange(10,180.0,30.0), dashes=[10, 5], linewidth=2)
            m.drawmeridians(np.arange(-170.0,170.0,10.0),  linewidth=2)
        elif projection=='lambert':
            distEW, az, baz = obspy.geodetics.gps2dist_azimuth(minlat, minlon, minlat, maxlon) # distance is in m
            distNS, az, baz = obspy.geodetics.gps2dist_azimuth(minlat, minlon, maxlat+2., minlon) # distance is in m
            m       = Basemap(width=distEW, height=distNS, rsphere=(6378137.00,6356752.3142), resolution='h', projection='lcc',\
                        lat_1=minlat, lat_2=maxlat, lon_0=lon_centre, lat_0=lat_centre+1)
            m.drawparallels(np.arange(-80.0,80.0,10.0), linewidth=1, dashes=[2,2], labels=[1,1,0,0], fontsize=20)
            m.drawmeridians(np.arange(-170.0,170.0,10.0), linewidth=1, dashes=[2,2], labels=[0,0,1,0], fontsize=20)
            # m.drawparallels(np.arange(-80.0,80.0,10.0), linewidth=0.5, dashes=[2,2], labels=[1,0,0,0], fontsize=5)
            # m.drawmeridians(np.arange(-170.0,170.0,10.0), linewidth=0.5, dashes=[2,2], labels=[0,0,0,1], fontsize=5)
        m.drawcoastlines(linewidth=1.0)
        m.drawcountries(linewidth=1.)
        # # m.drawmapboundary(fill_color=[1.0,1.0,1.0])
        # m.fillcontinents(lake_color='#99ffff',zorder=0.2)
        # # m.drawlsmask(land_color='0.8', ocean_color='#99ffff')
        # m.drawmapboundary(fill_color="white")
        # m.shadedrelief(scale=1., origin='lower')
        try:
            geopolygons.PlotPolygon(inbasemap=m)
        except:
            pass
        return m
        
    
    def plot_paraval(self, pindex, org_mask=False, dtype='avg', ingrdfname=None, isthk=False, shpfx=None,\
            clabel='', cmap='cv', projection='lambert', hillshade=False, geopolygons=None, vmin=None, vmax=None, showfig=True):
        # # # mask        = self.attrs['mask']
        data, mask  = self.get_paraval(pindex=pindex, dtype=dtype, ingrdfname=ingrdfname, isthk=isthk)
        if org_mask:
            mask    = self.attrs['mask']
        mdata       = ma.masked_array(data, mask=mask )
        #-----------
        # plot data
        #-----------
        m           = self._get_basemap(projection=projection)
        x, y        = m(self.lonArr, self.latArr)
        shapefname  = '/projects/life9360/geological_maps/qfaults'
        m.readshapefile(shapefname, 'faultline', linewidth=2, color='grey')
        shapefname  = '/projects/life9360/AKgeol_web_shp/AKStategeolarc_generalized_WGS84'
        m.readshapefile(shapefname, 'geolarc', linewidth=1, color='grey')
        # shapefname  = '../AKfaults/qfaults'
        # m.readshapefile(shapefname, 'faultline', linewidth=2, color='grey')
        # shapefname  = '../AKgeol_web_shp/AKStategeolarc_generalized_WGS84'
        # m.readshapefile(shapefname, 'geolarc', linewidth=1, color='grey')
        # shapefname  = '/projects/life9360/AK_sediments/Cook_Inlet_sediments_WGS84'
        # m.readshapefile(shapefname, 'faultline', linewidth=1, color='blue')
        
        if cmap == 'ses3d':
            cmap        = colormaps.make_colormap({0.0:[0.1,0.0,0.0], 0.2:[0.8,0.0,0.0], 0.3:[1.0,0.7,0.0],0.48:[0.92,0.92,0.92],
                            0.5:[0.92,0.92,0.92], 0.52:[0.92,0.92,0.92], 0.7:[0.0,0.6,0.7], 0.8:[0.0,0.0,0.8], 1.0:[0.0,0.0,0.1]})
        elif cmap == 'cv':
            import pycpt
            cmap    = pycpt.load.gmtColormap('./cv.cpt')
        else:
            try:
                if os.path.isfile(cmap):
                    import pycpt
                    cmap    = pycpt.load.gmtColormap(cmap)
            except:
                pass
        ################################3
        if hillshade:
            from netCDF4 import Dataset
            from matplotlib.colors import LightSource
        
            etopodata   = Dataset('/projects/life9360/station_map/grd_dir/ETOPO2v2g_f4.nc')
            etopo       = etopodata.variables['z'][:]
            lons        = etopodata.variables['x'][:]
            lats        = etopodata.variables['y'][:]
            ls          = LightSource(azdeg=315, altdeg=45)
            # nx          = int((m.xmax-m.xmin)/40000.)+1; ny = int((m.ymax-m.ymin)/40000.)+1
            etopo,lons  = shiftgrid(180.,etopo,lons,start=False)
            # topodat,x,y = m.transform_scalar(etopo,lons,lats,nx,ny,returnxy=True)
            ny, nx      = etopo.shape
            topodat,xtopo,ytopo = m.transform_scalar(etopo,lons,lats,nx, ny, returnxy=True)
            m.imshow(ls.hillshade(topodat, vert_exag=1., dx=1., dy=1.), cmap='gray')
            mycm1=pycpt.load.gmtColormap('/projects/life9360/station_map/etopo1.cpt')
            mycm2=pycpt.load.gmtColormap('/projects/life9360/station_map/bathy1.cpt')
            mycm2.set_over('w',0)
            m.imshow(ls.shade(topodat, cmap=mycm1, vert_exag=1., dx=1., dy=1., vmin=0, vmax=8000))
            m.imshow(ls.shade(topodat, cmap=mycm2, vert_exag=1., dx=1., dy=1., vmin=-11000, vmax=-0.5))
        ###################################################################
        if hillshade:
            m.fillcontinents(lake_color='#99ffff',zorder=0.2, alpha=0.2)
        else:
            m.fillcontinents(lake_color='#99ffff',zorder=0.2)
        if hillshade:
            im          = m.pcolormesh(x, y, mdata, cmap=cmap, shading='gouraud', vmin=vmin, vmax=vmax, alpha=.5)
        else:
            im          = m.pcolormesh(x, y, mdata, cmap=cmap, shading='gouraud', vmin=vmin, vmax=vmax)
        cb          = m.colorbar(im, "bottom", size="3%", pad='2%')
        cb.set_label(clabel, fontsize=12, rotation=0)
        cb.ax.tick_params(labelsize=15)
        cb.set_alpha(1)
        cb.draw_all()
        # # cb.solids.set_rasterized(True)
        cb.solids.set_edgecolor("face")
        # m.shadedrelief(scale=1., origin='lower')
        if showfig:
            plt.show()
        return
    
    def plot(self, runtype, runid, datatype, period, shpfx=None, clabel='', cmap='cv', projection='lambert', hillshade=False,\
             geopolygons=None, vmin=None, vmax=None, showfig=True):
        """plot maps from the tomographic inversion
        =================================================================================================================
        ::: input parameters :::
        runtype         - type of run (0 - smooth run, 1 - quality controlled run)
        runid           - id of run
        datatype        - datatype for plotting
        period          - period of data
        clabel          - label of colorbar
        cmap            - colormap
        projection      - projection type
        geopolygons     - geological polygons for plotting
        vmin, vmax      - min/max value of plotting
        showfig         - show figure or not
        =================================================================================================================
        """
        # vdict       = {'ph': 'C', 'gr': 'U'}
        # datatype    = datatype.lower()
        rundict     = {0: 'smooth_run', 1: 'qc_run'}
        dataid      = rundict[runtype]+'_'+str(runid)
        self._get_lon_lat_arr(dataid)
        try:
            ingroup     = self['reshaped_'+dataid]
        except KeyError:
            try:
                self.creat_reshape_data(runtype=runtype, runid=runid)
                ingroup = self['reshaped_'+dataid]
            except KeyError:
                raise KeyError(dataid+ ' not exists!')
        pers        = self.attrs['period_array']
        if not period in pers:
            raise KeyError('period = '+str(period)+' not included in the database')
        pergrp  = ingroup['%g_sec'%( period )]
        if runtype == 1:
            isotropic   = ingroup.attrs['isotropic']
        else:
            isotropic   = True
        if datatype == 'vel' or datatype=='velocity' or datatype == 'v':
            if isotropic:
                datatype    = 'velocity'
            else:
                datatype    = 'vel_iso'
        if datatype == 'un' or datatype=='sem' or datatype == 'vel_sem':
            datatype        = 'vel_sem'
        try:
            data    = pergrp[datatype].value
        except:
            outstr      = ''
            for key in pergrp.keys():
                outstr  +=key
                outstr  +=', '
            outstr      = outstr[:-1]
            raise KeyError('Unexpected datatype: '+datatype+\
                           ', available datatypes are: '+outstr)
        if datatype == 'amp2':
            data    = data*100.
        if datatype == 'vel_sem':
            data    = data*1000.
        if not isotropic:
            if datatype == 'cone_radius' or datatype == 'gauss_std' or datatype == 'max_resp' or datatype == 'ncone' or \
                         datatype == 'ngauss' or datatype == 'vel_sem':
                mask    = ingroup['mask2']
            else:
                mask    = ingroup['mask1']
            mdata       = ma.masked_array(data, mask=mask )
        else:
            mdata       = data.copy()
        #-----------
        # plot data
        #-----------
        m           = self._get_basemap(projection=projection, geopolygons=geopolygons)
        x, y        = m(self.lonArr, self.latArr)
        # shapefname  = '/projects/life9360/geological_maps/qfaults'
        # m.readshapefile(shapefname, 'faultline', linewidth=2, color='blue')
        # shapefname  = '/projects/life9360/AKgeol_web_shp/AKStategeolarc_generalized_WGS84'
        # m.readshapefile(shapefname, 'geolarc', linewidth=1, color='blue')
        shapefname  = '../AKfaults/qfaults'
        m.readshapefile(shapefname, 'faultline', linewidth=2, color='grey')
        shapefname  = '../AKgeol_web_shp/AKStategeolarc_generalized_WGS84'
        m.readshapefile(shapefname, 'geolarc', linewidth=1, color='grey')
        # shapefname  = '/projects/life9360/AK_sediments/Cook_Inlet_sediments_WGS84'
        # m.readshapefile(shapefname, 'faultline', linewidth=1, color='blue')
        
        if cmap == 'ses3d':
            cmap        = colormaps.make_colormap({0.0:[0.1,0.0,0.0], 0.2:[0.8,0.0,0.0], 0.3:[1.0,0.7,0.0],0.48:[0.92,0.92,0.92],
                            0.5:[0.92,0.92,0.92], 0.52:[0.92,0.92,0.92], 0.7:[0.0,0.6,0.7], 0.8:[0.0,0.0,0.8], 1.0:[0.0,0.0,0.1]})
        elif cmap == 'cv':
            import pycpt
            cmap    = pycpt.load.gmtColormap('./cv.cpt')
        else:
            try:
                if os.path.isfile(cmap):
                    import pycpt
                    cmap    = pycpt.load.gmtColormap(cmap)
            except:
                pass
        ################################3
        if hillshade:
            from netCDF4 import Dataset
            from matplotlib.colors import LightSource
        
            etopodata   = Dataset('/projects/life9360/station_map/grd_dir/ETOPO2v2g_f4.nc')
            etopo       = etopodata.variables['z'][:]
            lons        = etopodata.variables['x'][:]
            lats        = etopodata.variables['y'][:]
            ls          = LightSource(azdeg=315, altdeg=45)
            # nx          = int((m.xmax-m.xmin)/40000.)+1; ny = int((m.ymax-m.ymin)/40000.)+1
            etopo,lons  = shiftgrid(180.,etopo,lons,start=False)
            # topodat,x,y = m.transform_scalar(etopo,lons,lats,nx,ny,returnxy=True)
            ny, nx      = etopo.shape
            topodat,xtopo,ytopo = m.transform_scalar(etopo,lons,lats,nx, ny, returnxy=True)
            m.imshow(ls.hillshade(topodat, vert_exag=1., dx=1., dy=1.), cmap='gray')
            mycm1=pycpt.load.gmtColormap('/projects/life9360/station_map/etopo1.cpt')
            mycm2=pycpt.load.gmtColormap('/projects/life9360/station_map/bathy1.cpt')
            mycm2.set_over('w',0)
            m.imshow(ls.shade(topodat, cmap=mycm1, vert_exag=1., dx=1., dy=1., vmin=0, vmax=8000))
            m.imshow(ls.shade(topodat, cmap=mycm2, vert_exag=1., dx=1., dy=1., vmin=-11000, vmax=-0.5))
        ###################################################################
        if hillshade:
            m.fillcontinents(lake_color='#99ffff',zorder=0.2, alpha=0.2)
        else:
            m.fillcontinents(lake_color='#99ffff',zorder=0.2)
        if hillshade:
            im          = m.pcolormesh(x, y, mdata, cmap=cmap, shading='gouraud', vmin=vmin, vmax=vmax, alpha=.5)
        else:
            im          = m.pcolormesh(x, y, mdata, cmap=cmap, shading='gouraud', vmin=vmin, vmax=vmax)
        cb          = m.colorbar(im, "bottom", size="3%", pad='2%')
        cb.set_label(clabel, fontsize=12, rotation=0)
        plt.suptitle(str(period)+' sec', fontsize=20)
        cb.ax.tick_params(labelsize=15)
        cb.set_alpha(1)
        cb.draw_all()
        print 'plotting data from '+dataid
        # # cb.solids.set_rasterized(True)
        cb.solids.set_edgecolor("face")
        m.shadedrelief(scale=1., origin='lower')
        if showfig:
            plt.show()
        return
    # def plot_paraval(self, pindex):
        
    
# # #             
# # #     def mc_inv_iso_mp(self, instafname=None, ref=True, phase=True, group=False, outdir='./workingdir', dispdtype='ph', wdisp=0.2, rffactor=40.,\
# # #                    monoc=True, verbose=False, step4uwalk=2500, numbrun=10000, subsize=1000, nprocess=None):
# # #         if not os.path.isdir(outdir):
# # #             os.makedirs(outdir)
# # #         if instafname is None:
# # #             stalst  = self.waveforms.list()
# # #         else:
# # #             stalst  = []
# # #             with open(instafname, 'r') as fid:
# # #                 for line in fid.readlines():
# # #                     sline   = line.split()
# # #                     if sline[2] == '1':
# # #                         stalst.append(sline[0])
# # #         #-------------------------
# # #         # prepare data
# # #         #-------------------------
# # #         vpr_lst = []
# # #         ista    = 0
# # #         Nsta    = len(stalst)
# # #         for staid in stalst:
# # #             netcode, stacode    = staid.split('.')
# # #             staid_aux           = netcode+'_'+stacode
# # #             stla, elev, stlo    = self.waveforms[staid].coordinates.values()
# # #             #-----------------------------
# # #             # get data
# # #             #-----------------------------
# # #             vpr                 = vprofile.vprofile1d()
# # #             if phase:
# # #                 try:
# # #                     indisp      = self.auxiliary_data['RayDISPcurve']['ray']['ph'][staid_aux].data.value
# # #                     vpr.get_disp(indata=indisp, dtype='ph', wtype='ray')
# # #                 except KeyError:
# # #                     print 'WARNING: No phase dispersion data for station: '+staid
# # #             if group:
# # #                 try:
# # #                     indisp      = self.auxiliary_data['RayDISPcurve']['ray']['gr'][staid_aux].data.value
# # #                     vpr.get_disp(indata=indisp, dtype='gr', wtype='ray')
# # #                 except KeyError:
# # #                     print 'WARNING: No group dispersion data for station: '+staid
# # #             if vpr.data.dispR.npper == 0 and vpr.data.dispR.ngper == 0:
# # #                 print 'WARNING: No dispersion data for station: '+staid 
# # #                 continue
# # #             if ref:
# # #                 try:
# # #                     inrf        = self.auxiliary_data['RefR'][staid_aux+'_P'].data.value
# # #                     N           = self.auxiliary_data['RefR'][staid_aux+'_P'].parameters['npts']
# # #                     dt          = self.auxiliary_data['RefR'][staid_aux+'_P'].parameters['delta']
# # #                     indata      = np.zeros((3, N))
# # #                     indata[0, :]= np.arange(N)*dt
# # #                     indata[1, :]= inrf[0, :]
# # #                     indata[2, :]= inrf[3, :]
# # #                     vpr.get_rf(indata = indata)
# # #                 except KeyError:
# # #                     print 'WARNING: No phase dispersion data for station: '+staid
# # #             #-----------------------------
# # #             # initial model parameters
# # #             #-----------------------------
# # #             vsdata              = self.auxiliary_data['ReferenceModel'][staid_aux].data.value
# # #             crtthk           = self.auxiliary_data['MohoDepth'][staid_aux].parameters['moho_depth']
# # #             sedthk            = self.auxiliary_data['SediDepth'][staid_aux].parameters['sedi_depth']
# # #             vpr.model.isomod.parameterize_input(zarr=vsdata[:, 0], vsarr=vsdata[:, 1], crtthk=crtthk, sedthk=sedthk, maxdepth=200.)
# # #             vpr.getpara()
# # #             vpr.staid           = staid
# # #             ista                += 1
# # #             vpr_lst.append(vpr)
# # #         #----------------------------------------
# # #         # Joint inversion with multiprocessing
# # #         #----------------------------------------
# # #         print 'Start MC joint inversion, '+time.ctime()
# # #         stime   = time.time()
# # #         if Nsta > subsize:
# # #             Nsub                = int(len(vpr_lst)/subsize)
# # #             for isub in xrange(Nsub):
# # #                 print 'Subset:', isub,'in',Nsub,'sets'
# # #                 cvpr_lst        = vpr_lst[isub*subsize:(isub+1)*subsize]
# # #                 MCINV           = partial(mc4mp, outdir=outdir, dispdtype=dispdtype, wdisp=wdisp, rffactor=rffactor,\
# # #                                     monoc=monoc, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
# # #                 pool            = multiprocessing.Pool(processes=nprocess)
# # #                 pool.map(MCINV, cvpr_lst) #make our results with a map call
# # #                 pool.close() #we are not adding any more processes
# # #                 pool.join() #tell it to wait until all threads are done before going on
# # #             cvpr_lst            = vpr_lst[(isub+1)*subsize:]
# # #             MCINV               = partial(mc4mp, outdir=outdir, dispdtype=dispdtype, wdisp=wdisp, rffactor=rffactor,\
# # #                                     monoc=monoc, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
# # #             pool                = multiprocessing.Pool(processes=nprocess)
# # #             pool.map(MCINV, cvpr_lst) #make our results with a map call
# # #             pool.close() #we are not adding any more processes
# # #             pool.join() #tell it to wait until all threads are done before going on
# # #         else:
# # #             MCINV               = partial(mc4mp, outdir=outdir, dispdtype=dispdtype, wdisp=wdisp, rffactor=rffactor,\
# # #                                     monoc=monoc, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
# # #             pool                = multiprocessing.Pool(processes=nprocess)
# # #             pool.map(MCINV, vpr_lst) #make our results with a map call
# # #             pool.close() #we are not adding any more processes
# # #             pool.join() #tell it to wait until all threads are done before going on
# # #             
# # #             # if staid != 'AK.MCK': continue
# # #             # print '--- Joint MC inversion for station: '+staid+' '+str(ista)+'/'+str(Nsta)
# # #             # vpr.mc_joint_inv_iso(outdir=outdir, pfx = staid, rffactor=5., wdisp=0.1)
# # #             # vpr.mc_joint_inv_iso(outdir=outdir, pfx = staid)
# # #             # if staid == 'AK.COLD':
# # #             #     return vpr
# # #         print 'End MC joint inversion, '+time.ctime()
# # #         etime   = time.time()
# # #         print 'Elapsed time: '+str(etime-stime)+' secs'
# # #         return
# # #     
# # # 
# # #     
# # # 
# # # def mc4mp(invpr, outdir, dispdtype, wdisp, rffactor, monoc, verbose, step4uwalk, numbrun):
# # #     print '--- Joint MC inversion for station: '+invpr.staid
# # #     invpr.mc_joint_inv_iso(outdir=outdir, wdisp=wdisp, rffactor=rffactor,\
# # #                    monoc=monoc, pfx=invpr.staid, verbose=verbose, step4uwalk=step4uwalk, numbrun=numbrun)
# # #     return


    